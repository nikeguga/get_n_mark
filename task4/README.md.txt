# Task 4.2 (Remade)

**Task 4.2 (remade)** — это обновлённая версия кода для парсинга новостей с сайта [news.mail.ru](https://news.mail.ru/), которая использует XPath для извлечения текстового содержимого статей. 

## История проекта:

- **Task 4**: Изначально разработанный файл, предлагавшийся к проверке, в котором осуществлялся скрэппинг главной страницы новостей, но содержимое статей не загружалось.
- **Task 4.1**: Предназначался для проверки корректности загрузки данных в CSV файл.
- **Task 4.3 (check)**: Быстрая проверка содержимого CSV файла для убедительности в правильности работы.

На данный момент к проверке предлагается версия **Task 4.2 (remade)**.

## Описание кода Task 4.2 (Remade):

### Функционал:

1. **Сбор заголовков и ссылок на статьи**:
   - С главной страницы [news.mail.ru](https://news.mail.ru/) скрэпятся заголовки новостей и ссылки на статьи.
   
2. **Парсинг текста статей**:
   - Для каждой статьи происходит отдельный HTTP-запрос для получения её полного текста.
   - Используется XPath для извлечения всех текстовых параграфов статьи. Текст статьи находится в блоке с атрибутом `data-qa="Text"`.
   
3. **Сохранение в CSV**:
   - Собранные данные сохраняются в CSV файл с заголовками: `Title`, `Link`, `Article Content`.
   - Путь для сохранения CSV файла — `C:\Users\User\Desktop\Get_n_mark_data\task4\news_mail_ru.csv`.

### Используемые технологии:

- **requests**: Для отправки HTTP-запросов к сайту.
- **lxml**: Для парсинга HTML содержимого и извлечения данных с помощью XPath.
- **csv**: Для записи данных в файл CSV.

### Структура данных:

- **Title**: Заголовок новости.
- **Link**: Ссылка на новость.
- **Article Content**: Полный текст статьи.

### Описание шагов:

1. **Получение данных главной страницы**:
   - Выполняется запрос на главную страницу сайта.
   - Извлекаются все блоки новостей, содержащие ссылки и заголовки.
   
2. **Парсинг текста статей**:
   - Каждая статья открывается по отдельной ссылке.
   - Из статьи извлекается текст при помощи XPath `//div[@data-qa="Text"]//p/text()`, который позволяет получить весь текст внутри параграфов.

3. **Сохранение в CSV**:
   - Собранные данные загружаются в CSV файл, который содержит заголовки новостей, ссылки на них и полный текст статей.

### Как использовать:

1. Убедитесь, что установлены необходимые библиотеки:
   ```bash
   pip install requests lxml
